{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1vVWe0_uuyCh"
   },
   "source": [
    "# Generate Homographic Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With image and video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "HQfr0cmBu0C-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of video or unable to load video.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Global variables\n",
    "video_coords = []\n",
    "image_coords = []\n",
    "mode = \"video\"  # Start in video mode\n",
    "video_file = 0\n",
    "image_file = \"/Users/xs496-jassin/Desktop/Surveillance/cafeteria2.jpg\"\n",
    "video_output = \"video_coords.txt\"\n",
    "image_output = \"image_coords.txt\"\n",
    "homography_output = \"homography_matrix.txt\"\n",
    "h_matrix = None\n",
    "\n",
    "def save_coords_to_file(filename, coords):\n",
    "    \"\"\"Save coordinates to a file.\"\"\"\n",
    "    with open(filename, \"w\") as file:\n",
    "        for x, y in coords:\n",
    "            file.write(f\"{x}, {y}\\n\")\n",
    "    print(f\"Coordinates saved to {filename}\")\n",
    "\n",
    "def save_homography_to_file(filename, matrix):\n",
    "    \"\"\"Save homography matrix to a file.\"\"\"\n",
    "    with open(filename, \"w\") as file:\n",
    "        for row in matrix:\n",
    "            file.write(\" \".join(map(str, row)) + \"\\n\")\n",
    "    print(f\"Homography matrix saved to {filename}\")\n",
    "\n",
    "def click_event(event, x, y, flags, param):\n",
    "    \"\"\"Handle mouse click events to capture coordinates.\"\"\"\n",
    "    global video_coords, image_coords, mode\n",
    "\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        if mode == \"video\":\n",
    "            video_coords.append((x, y))\n",
    "            print(f\"Video coordinates: {x}, {y}\")\n",
    "        elif mode == \"image\":\n",
    "            image_coords.append((x, y))\n",
    "            print(f\"Image coordinates: {x}, {y}\")\n",
    "\n",
    "def compute_homography():\n",
    "    \"\"\"Compute the homography matrix from video to image.\"\"\"\n",
    "    global h_matrix\n",
    "    if len(video_coords) >= 4 and len(image_coords) >= 4:\n",
    "        video_pts = np.array(video_coords, dtype=np.float32)\n",
    "        image_pts = np.array(image_coords, dtype=np.float32)\n",
    "        h_matrix, _ = cv2.findHomography(video_pts, image_pts)\n",
    "        print(\"Homography matrix computed:\")\n",
    "        print(h_matrix)\n",
    "    else:\n",
    "        print(\"Need at least 4 points in both video and image to compute homography.\")\n",
    "\n",
    "def main():\n",
    "    global mode\n",
    "    # Load video and image\n",
    "    video = cv2.VideoCapture(video_file)\n",
    "    image = cv2.imread(image_file)\n",
    "\n",
    "    while True:\n",
    "        if mode == \"video\":\n",
    "            ret, frame = video.read()\n",
    "            if not ret:\n",
    "                print(\"End of video or unable to load video.\")\n",
    "                break\n",
    "            cv2.imshow(\"Frame\", frame)\n",
    "        else:  # mode == \"image\"\n",
    "            cv2.imshow(\"Frame\", image)\n",
    "\n",
    "        cv2.setMouseCallback(\"Frame\", click_event)\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('s'):  # Switch mode\n",
    "            mode = \"image\" if mode == \"video\" else \"video\"\n",
    "            print(f\"Switched to {mode} mode.\")\n",
    "        elif key == ord('q'):  # Quit\n",
    "            break\n",
    "        elif key == ord('c'):  # Compute homography matrix\n",
    "            compute_homography()\n",
    "        elif key == ord('v'):  # Save video coordinates\n",
    "            save_coords_to_file(video_output, video_coords)\n",
    "        elif key == ord('i'):  # Save image coordinates\n",
    "            save_coords_to_file(image_output, image_coords)\n",
    "        elif key == ord('h'):  # Save homography matrix\n",
    "            if h_matrix is not None:\n",
    "                save_homography_to_file(homography_output, h_matrix)\n",
    "            else:\n",
    "                print(\"Homography matrix not computed yet. Press 'c' to compute.\")\n",
    "\n",
    "    video.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Image 1 and Image 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MG1SxWhyu29Q"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-14 12:35:24.612 Python[19466:35648182] +[IMKClient subclass]: chose IMKClient_Legacy\n",
      "2025-01-14 12:35:24.612 Python[19466:35648182] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Global variables\n",
    "image1_coords = []\n",
    "image2_coords = []\n",
    "mode = \"image1\"  # Start in image1 mode\n",
    "image1_file = \"/Users/xs496-jassin/Desktop/Surveillance/640sizecv2.png\"  # Image 1 file path\n",
    "image2_file = \"/Users/xs496-jassin/Desktop/Surveillance/grid.jpg\"  # Image 2 file path\n",
    "image1_output = \"image1_coords.txt\"\n",
    "image2_output = \"image2_coords.txt\"\n",
    "homography_output = \"homography_matrix.txt\"\n",
    "h_matrix = None\n",
    "\n",
    "def save_coords_to_file(filename, coords):\n",
    "    \"\"\"Save coordinates to a file.\"\"\"\n",
    "    with open(filename, \"w\") as file:\n",
    "        for x, y in coords:\n",
    "            file.write(f\"{x}, {y}\\n\")\n",
    "    print(f\"Coordinates saved to {filename}\")\n",
    "\n",
    "def save_homography_to_file(filename, matrix):\n",
    "    \"\"\"Save homography matrix to a file.\"\"\"\n",
    "    with open(filename, \"w\") as file:\n",
    "        for row in matrix:\n",
    "            file.write(\" \".join(map(str, row)) + \"\\n\")\n",
    "    print(f\"Homography matrix saved to {filename}\")\n",
    "\n",
    "def click_event(event, x, y, flags, param):\n",
    "    \"\"\"Handle mouse click events to capture coordinates.\"\"\"\n",
    "    global image1_coords, image2_coords, mode\n",
    "\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        if mode == \"image1\":\n",
    "            image1_coords.append((x, y))\n",
    "            print(f\"Image 1 coordinates: {x}, {y}\")\n",
    "        elif mode == \"image2\":\n",
    "            image2_coords.append((x, y))\n",
    "            print(f\"Image 2 coordinates: {x}, {y}\")\n",
    "\n",
    "def compute_homography():\n",
    "    \"\"\"Compute the homography matrix from image1 to image2.\"\"\"\n",
    "    global h_matrix\n",
    "    if len(image1_coords) >= 4 and len(image2_coords) >= 4:\n",
    "        image1_pts = np.array(image1_coords, dtype=np.float32)\n",
    "        image2_pts = np.array(image2_coords, dtype=np.float32)\n",
    "        h_matrix, _ = cv2.findHomography(image1_pts, image2_pts)\n",
    "        print(\"Homography matrix computed:\")\n",
    "        print(h_matrix)\n",
    "    else:\n",
    "        print(\"Need at least 4 points in both images to compute homography.\")\n",
    "\n",
    "def main():\n",
    "    global mode\n",
    "    # Load both images\n",
    "    image1 = cv2.imread(image1_file)\n",
    "    image2 = cv2.imread(image2_file)\n",
    "\n",
    "    while True:\n",
    "        if mode == \"image1\":\n",
    "            cv2.imshow(\"Frame\", image1)\n",
    "        elif mode == \"image2\":\n",
    "            cv2.imshow(\"Frame\", image2)\n",
    "\n",
    "        cv2.setMouseCallback(\"Frame\", click_event)\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('s'):  # Switch mode between image1 and image2\n",
    "            mode = \"image2\" if mode == \"image1\" else \"image1\"\n",
    "            print(f\"Switched to {mode} mode.\")\n",
    "        elif key == ord('q'):  # Quit\n",
    "            break\n",
    "        elif key == ord('c'):  # Compute homography matrix\n",
    "            compute_homography()\n",
    "        elif key == ord('i'):  # Save image1 coordinates\n",
    "            save_coords_to_file(image1_output, image1_coords)\n",
    "        elif key == ord('j'):  # Save image2 coordinates\n",
    "            save_coords_to_file(image2_output, image2_coords)\n",
    "        elif key == ord('h'):  # Save homography matrix\n",
    "            if h_matrix is not None:\n",
    "                save_homography_to_file(homography_output, h_matrix)\n",
    "            else:\n",
    "                print(\"Homography matrix not computed yet. Press 'c' to compute.\")\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48mnyuq-u3Wr"
   },
   "source": [
    "# Point Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wRe-1vPEu5N8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Homography Matrix:\n",
      "[[-4.79612391e-01  6.37355027e-01  7.55058868e+02]\n",
      " [-9.01731146e-04 -2.66667584e-01  4.31411733e+02]\n",
      " [-1.72981056e-04  1.42417329e-03  1.00000000e+00]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-11 09:52:01.039 Python[92782:23134421] +[IMKClient subclass]: chose IMKClient_Legacy\n",
      "2025-01-11 09:52:01.039 Python[92782:23134421] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n",
      "/var/folders/lq/b9jl13d10hq6lrw9wtvydrj40000gp/T/ipykernel_92782/3334803666.py:14: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  return int(image_coords[0]), int(image_coords[1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapped coordinates: Video(1277, 17) -> Image(190, 529)\n",
      "Mapped coordinates: Video(103, 609) -> Image(591, 145)\n",
      "Mapped coordinates: Video(271, 624) -> Image(555, 143)\n",
      "Mapped coordinates: Video(510, 633) -> Image(504, 144)\n",
      "Mapped coordinates: Video(673, 658) -> Image(467, 140)\n",
      "Mapped coordinates: Video(1150, 669) -> Image(359, 143)\n",
      "Mapped coordinates: Video(1470, 670) -> Image(280, 147)\n",
      "Mapped coordinates: Video(1618, 668) -> Image(242, 150)\n",
      "Mapped coordinates: Video(529, 916) -> Image(490, 84)\n",
      "Mapped coordinates: Video(786, 929) -> Image(443, 83)\n",
      "Mapped coordinates: Video(1139, 936) -> Image(377, 84)\n",
      "Mapped coordinates: Video(1517, 916) -> Image(299, 90)\n",
      "Mapped coordinates: Video(1657, 589) -> Image(216, 175)\n",
      "Mapped coordinates: Video(1704, 641) -> Image(214, 160)\n",
      "Mapped coordinates: Video(1749, 710) -> Image(215, 140)\n",
      "Mapped coordinates: Video(1802, 767) -> Image(213, 126)\n",
      "Mapped coordinates: Video(1847, 829) -> Image(213, 112)\n",
      "Mapped coordinates: Video(1891, 885) -> Image(213, 100)\n",
      "Failed to grab frame from the camera.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Initialize global variables\n",
    "h_matrix = None\n",
    "image_file = \"/Users/xs496-jassin/Desktop/Surveillance/room.jpg\"\n",
    "image = None\n",
    "\n",
    "def transform_to_image(x, y, h_matrix):\n",
    "    \"\"\"Transform video coordinates to image coordinates using the homography matrix.\"\"\"\n",
    "    video_coords = np.array([[x, y, 1]]).T  # Homogeneous coordinates\n",
    "    image_coords = np.dot(h_matrix, video_coords)\n",
    "    image_coords /= image_coords[2]  # Normalize by z-coordinate\n",
    "    return int(image_coords[0]), int(image_coords[1])\n",
    "\n",
    "def click_event(event, x, y, flags, param):\n",
    "    \"\"\"Handle mouse click events to draw a circle on the image.\"\"\"\n",
    "    global image, h_matrix\n",
    "\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        if h_matrix is None:\n",
    "            print(\"Homography matrix is not loaded or computed. Please set it up.\")\n",
    "            return\n",
    "\n",
    "        # Transform video coordinates to image coordinates\n",
    "        img_x, img_y = transform_to_image(x, y, h_matrix)\n",
    "        print(f\"Mapped coordinates: Video({x}, {y}) -> Image({img_x}, {img_y})\")\n",
    "\n",
    "        # Draw a circle on the image\n",
    "        cv2.circle(image, (img_x, img_y), 5, (0, 0, 255), -1)\n",
    "        cv2.imshow(\"Mapped Image\", image)\n",
    "\n",
    "def main():\n",
    "    global image, h_matrix\n",
    "\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_file)\n",
    "    if image is None:\n",
    "        print(\"Failed to load the image. Check the image path.\")\n",
    "        return\n",
    "\n",
    "    # Load homography matrix\n",
    "    h_matrix = np.loadtxt(\"homography_matrix.txt\")  # Load the previously saved homography matrix\n",
    "    print(\"Loaded Homography Matrix:\")\n",
    "    print(h_matrix)\n",
    "\n",
    "    # Open the camera\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Failed to open the camera.\")\n",
    "        return\n",
    "\n",
    "    # Show the image for mapping\n",
    "    cv2.imshow(\"Mapped Image\", image)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to grab frame from the camera.\")\n",
    "            break\n",
    "\n",
    "        # Show the video stream\n",
    "        cv2.imshow(\"Camera Feed\", frame)\n",
    "\n",
    "        # Set mouse callback only after the \"Camera Feed\" window is created\n",
    "        cv2.setMouseCallback(\"Camera Feed\", click_event)\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q'):  # Quit the program\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "    cv2.imshow(\"Mapped Image\", image)  # Display the image for mapping\n",
    "    cv2.setMouseCallback(\"Camera Feed\", click_event)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to grab frame from the camera.\")\n",
    "            break\n",
    "\n",
    "        # Show the video stream\n",
    "        cv2.imshow(\"Camera Feed\", frame)\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q'):  # Quit the program\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x1laeS1xu6Fu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2La7VqufxZSq"
   },
   "source": [
    "# Dotted tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dvnqJI4HxYzb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Homography Matrix:\n",
      "[[-4.79612391e-01  6.37355027e-01  7.55058868e+02]\n",
      " [-9.01731146e-04 -2.66667584e-01  4.31411733e+02]\n",
      " [-1.72981056e-04  1.42417329e-03  1.00000000e+00]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-11 10:01:30.044 Python[1299:23161632] +[IMKClient subclass]: chose IMKClient_Legacy\n",
      "2025-01-11 10:01:30.044 Python[1299:23161632] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n",
      "/var/folders/lq/b9jl13d10hq6lrw9wtvydrj40000gp/T/ipykernel_1299/1239025518.py:15: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  return int(image_coords[0]), int(image_coords[1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapped coordinates: Video(219, 367) -> Image(595, 224)\n",
      "Mapped coordinates: Video(184, 438) -> Image(594, 197)\n",
      "Mapped coordinates: Video(160, 489) -> Image(593, 180)\n",
      "Mapped coordinates: Video(171, 559) -> Image(582, 159)\n",
      "Mapped coordinates: Video(263, 607) -> Image(558, 148)\n",
      "Mapped coordinates: Video(373, 600) -> Image(535, 151)\n",
      "Mapped coordinates: Video(540, 596) -> Image(498, 154)\n",
      "Mapped coordinates: Video(690, 582) -> Image(465, 161)\n",
      "Mapped coordinates: Video(826, 577) -> Image(432, 164)\n",
      "Mapped coordinates: Video(968, 566) -> Image(397, 170)\n",
      "Mapped coordinates: Video(1108, 559) -> Image(361, 175)\n",
      "Mapped coordinates: Video(1243, 554) -> Image(325, 179)\n",
      "Mapped coordinates: Video(1498, 541) -> Image(252, 189)\n",
      "Mapped coordinates: Video(1567, 547) -> Image(233, 188)\n",
      "Mapped coordinates: Video(1596, 611) -> Image(237, 167)\n",
      "Mapped coordinates: Video(1605, 648) -> Image(242, 156)\n",
      "Mapped coordinates: Video(1606, 708) -> Image(251, 139)\n",
      "Mapped coordinates: Video(1606, 791) -> Image(264, 118)\n",
      "Mapped coordinates: Video(1581, 874) -> Image(280, 99)\n",
      "Mapped coordinates: Video(1508, 930) -> Image(302, 88)\n",
      "Mapped coordinates: Video(1348, 969) -> Image(338, 80)\n",
      "Mapped coordinates: Video(1148, 964) -> Image(376, 79)\n",
      "Mapped coordinates: Video(919, 956) -> Image(419, 79)\n",
      "Mapped coordinates: Video(775, 948) -> Image(445, 80)\n",
      "Mapped coordinates: Video(667, 933) -> Image(465, 82)\n",
      "Mapped coordinates: Video(610, 927) -> Image(475, 82)\n",
      "Mapped coordinates: Video(561, 925) -> Image(484, 82)\n",
      "Mapped coordinates: Video(488, 920) -> Image(497, 83)\n",
      "Mapped coordinates: Video(431, 917) -> Image(507, 83)\n",
      "Mapped coordinates: Video(364, 906) -> Image(519, 85)\n",
      "Mapped coordinates: Video(306, 902) -> Image(530, 85)\n",
      "Mapped coordinates: Video(251, 889) -> Image(540, 87)\n",
      "Mapped coordinates: Video(193, 870) -> Image(551, 90)\n",
      "Mapped coordinates: Video(145, 846) -> Image(561, 94)\n",
      "Mapped coordinates: Video(126, 810) -> Image(568, 100)\n",
      "Mapped coordinates: Video(82, 746) -> Image(581, 113)\n",
      "Mapped coordinates: Video(79, 693) -> Image(587, 124)\n",
      "Mapped coordinates: Video(62, 612) -> Image(599, 144)\n",
      "Mapped coordinates: Video(59, 542) -> Image(608, 162)\n",
      "Mapped coordinates: Video(63, 523) -> Image(610, 168)\n",
      "Mapped coordinates: Video(76, 496) -> Image(611, 176)\n",
      "Mapped coordinates: Video(98, 472) -> Image(609, 184)\n",
      "Mapped coordinates: Video(125, 442) -> Image(607, 194)\n",
      "Mapped coordinates: Video(144, 423) -> Image(605, 201)\n",
      "Mapped coordinates: Video(162, 410) -> Image(603, 206)\n",
      "Mapped coordinates: Video(194, 441) -> Image(591, 196)\n",
      "Mapped coordinates: Video(185, 478) -> Image(588, 184)\n",
      "Mapped coordinates: Video(187, 501) -> Image(585, 177)\n",
      "Mapped coordinates: Video(210, 588) -> Image(571, 152)\n",
      "Mapped coordinates: Video(219, 604) -> Image(567, 148)\n",
      "Mapped coordinates: Video(243, 654) -> Image(558, 135)\n",
      "Mapped coordinates: Video(273, 663) -> Image(551, 134)\n",
      "Mapped coordinates: Video(312, 677) -> Image(542, 131)\n",
      "Mapped coordinates: Video(375, 690) -> Image(529, 128)\n",
      "Mapped coordinates: Video(418, 699) -> Image(520, 127)\n",
      "Mapped coordinates: Video(600, 719) -> Image(482, 124)\n",
      "Mapped coordinates: Video(651, 726) -> Image(471, 123)\n",
      "Mapped coordinates: Video(844, 746) -> Image(430, 120)\n",
      "Mapped coordinates: Video(905, 732) -> Image(417, 124)\n",
      "Mapped coordinates: Video(992, 737) -> Image(398, 124)\n",
      "Mapped coordinates: Video(1080, 745) -> Image(379, 123)\n",
      "Mapped coordinates: Video(1228, 750) -> Image(347, 124)\n",
      "Mapped coordinates: Video(1312, 749) -> Image(327, 125)\n",
      "Mapped coordinates: Video(1397, 748) -> Image(308, 126)\n",
      "Mapped coordinates: Video(1467, 741) -> Image(290, 129)\n",
      "Mapped coordinates: Video(1416, 674) -> Image(294, 146)\n",
      "Mapped coordinates: Video(1168, 593) -> Image(348, 165)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Initialize global variables\n",
    "h_matrix = None\n",
    "image_file = \"/Users/xs496-jassin/Desktop/Surveillance/room.jpg\"\n",
    "image = None\n",
    "trajectory_points = []  # List to store trajectory points on the image\n",
    "\n",
    "def transform_to_image(x, y, h_matrix):\n",
    "    \"\"\"Transform video coordinates to image coordinates using the homography matrix.\"\"\"\n",
    "    video_coords = np.array([[x, y, 1]]).T  # Homogeneous coordinates\n",
    "    image_coords = np.dot(h_matrix, video_coords)\n",
    "    image_coords /= image_coords[2]  # Normalize by z-coordinate\n",
    "    return int(image_coords[0]), int(image_coords[1])\n",
    "\n",
    "def draw_dashed_curve(image, points, dash_length=10, gap_length=5):\n",
    "    \"\"\"Draw a dashed curve connecting the trajectory points.\"\"\"\n",
    "    for i in range(1, len(points)):\n",
    "        start_point = points[i - 1]\n",
    "        end_point = points[i]\n",
    "\n",
    "        # Compute the Euclidean distance between two points\n",
    "        distance = int(np.linalg.norm(np.array(end_point) - np.array(start_point)))\n",
    "\n",
    "        # Compute the number of dashes and gaps\n",
    "        dashes = distance // (dash_length + gap_length)\n",
    "        for j in range(dashes):\n",
    "            # Compute start and end of each dash\n",
    "            fraction_start = j * (dash_length + gap_length) / distance\n",
    "            fraction_end = (j * (dash_length + gap_length) + dash_length) / distance\n",
    "            pt1 = (\n",
    "                int(start_point[0] + fraction_start * (end_point[0] - start_point[0])),\n",
    "                int(start_point[1] + fraction_start * (end_point[1] - start_point[1])),\n",
    "            )\n",
    "            pt2 = (\n",
    "                int(start_point[0] + fraction_end * (end_point[0] - start_point[0])),\n",
    "                int(start_point[1] + fraction_end * (end_point[1] - start_point[1])),\n",
    "            )\n",
    "            # Draw the dash\n",
    "            cv2.line(image, pt1, pt2, (0, 0, 255), 2)\n",
    "\n",
    "def click_event(event, x, y, flags, param):\n",
    "    \"\"\"Handle mouse click events to update trajectory and draw on the image.\"\"\"\n",
    "    global trajectory_points, image, h_matrix\n",
    "\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        if h_matrix is None:\n",
    "            print(\"Homography matrix is not loaded or computed. Please set it up.\")\n",
    "            return\n",
    "\n",
    "        # Transform video coordinates to image coordinates\n",
    "        img_x, img_y = transform_to_image(x, y, h_matrix)\n",
    "        print(f\"Mapped coordinates: Video({x}, {y}) -> Image({img_x}, {img_y})\")\n",
    "\n",
    "        # Add the new point to the trajectory\n",
    "        trajectory_points.append((img_x, img_y))\n",
    "\n",
    "        # Clear and redraw the trajectory with dashed lines\n",
    "        temp_image = image.copy()\n",
    "        draw_dashed_curve(temp_image, trajectory_points)\n",
    "        cv2.imshow(\"Mapped Image\", temp_image)\n",
    "\n",
    "def main():\n",
    "    global image, h_matrix\n",
    "\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_file)\n",
    "    if image is None:\n",
    "        print(\"Failed to load the image. Check the image path.\")\n",
    "        return\n",
    "\n",
    "    # Load homography matrix\n",
    "    h_matrix = np.loadtxt(\"homography_matrix.txt\")  # Load the previously saved homography matrix\n",
    "    print(\"Loaded Homography Matrix:\")\n",
    "    print(h_matrix)\n",
    "\n",
    "    # Open the camera\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Failed to open the camera.\")\n",
    "        return\n",
    "\n",
    "    # Show the image for mapping\n",
    "    cv2.imshow(\"Mapped Image\", image)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to grab frame from the camera.\")\n",
    "            break\n",
    "\n",
    "        # Show the video stream\n",
    "        cv2.imshow(\"Camera Feed\", frame)\n",
    "\n",
    "        # Set mouse callback only after the \"Camera Feed\" window is created\n",
    "        cv2.setMouseCallback(\"Camera Feed\", click_event)\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q'):  # Quit the program\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2XH-Nil0xYki"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjIuhh_BFvIM"
   },
   "source": [
    "# Real Time Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "39ffHZWSFzJc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Homography Matrix:\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Initialize global variables\n",
    "h_matrix = None\n",
    "image_file = \"/Users/xs496-jassin/Desktop/Surveillance/nn.jpg\"\n",
    "image = None\n",
    "trajectory_points = []  # List to store trajectory points on the image\n",
    "threshold = 0.8  # Confidence threshold for person detection\n",
    "\n",
    "def transform_to_image(x, y, h_matrix):\n",
    "    \"\"\"Transform video coordinates to image coordinates using the homography matrix.\"\"\"\n",
    "    video_coords = np.array([[x, y, 1]]).T  # Homogeneous coordinates\n",
    "    image_coords = np.dot(h_matrix, video_coords)\n",
    "    image_coords /= image_coords[2]  # Normalize by z-coordinate\n",
    "    return int(image_coords[0]), int(image_coords[1])\n",
    "\n",
    "def draw_trajectory(image, points, color=(0, 0, 255), thickness=2):\n",
    "    \"\"\"Draw a trajectory connecting the points.\"\"\"\n",
    "    for i in range(1, len(points)):\n",
    "        cv2.line(image, points[i - 1], points[i], color, thickness)\n",
    "\n",
    "def process_frame(frame, model, threshold):\n",
    "    \"\"\"Detect persons in the frame and return the bottom center of their bounding boxes.\"\"\"\n",
    "    results = model.predict(frame, conf=threshold, classes=0, verbose=False)  # Class 0 is 'person'\n",
    "    centers = []\n",
    "\n",
    "    for bbox in results[0].boxes.xyxy:  # Bounding box format: (x1, y1, x2, y2)\n",
    "        x1, y1, x2, y2 = map(int, bbox)\n",
    "        bottom_center = ((x1 + x2) // 2, y2)  # Bottom-center of the bounding box\n",
    "        centers.append(bottom_center)\n",
    "\n",
    "    return centers\n",
    "\n",
    "def main():\n",
    "    global image, h_matrix, trajectory_points\n",
    "\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_file)\n",
    "    if image is None:\n",
    "        print(\"Failed to load the image. Check the image path.\")\n",
    "        return\n",
    "\n",
    "    # Load homography matrix\n",
    "    h_matrix = np.loadtxt(\"homography_matrix_grid.txt\")\n",
    "    print(\"Loaded Homography Matrix:\")\n",
    "\n",
    "    # Load YOLO model\n",
    "    model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "    # Open the camera\n",
    "    cap = cv2.VideoCapture(\"/Users/xs496-jassin/Desktop/Surveillance/cafe4.mov\")\n",
    "    if not cap.isOpened():\n",
    "        print(\"Failed to open the camera.\")\n",
    "        return\n",
    "\n",
    "    # Show the image for mapping\n",
    "    cv2.imshow(\"Mapped Image\", image)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to grab frame from the camera.\")\n",
    "            break\n",
    "\n",
    "        # Detect persons and get their bottom-center points\n",
    "        centers = process_frame(frame, model, threshold)\n",
    "\n",
    "        # Map each detected point to the image\n",
    "        for center in centers:\n",
    "            mapped_point = transform_to_image(center[0], center[1], h_matrix)\n",
    "            trajectory_points.append(mapped_point)\n",
    "\n",
    "        # Clear and redraw the trajectory\n",
    "        temp_image = image.copy()\n",
    "        draw_trajectory(temp_image, trajectory_points)\n",
    "\n",
    "        # Resize the frames to the same height for concatenation\n",
    "        frame_height, frame_width = frame.shape[:2]\n",
    "        image_height, image_width = temp_image.shape[:2]\n",
    "\n",
    "        scale = frame_height / image_height\n",
    "        resized_image = cv2.resize(temp_image, (int(image_width * scale), frame_height))\n",
    "\n",
    "        # Horizontally concatenate the video frame and the resized image\n",
    "        concatenated_frame = np.hstack((frame, resized_image))\n",
    "\n",
    "        # Display the concatenated frame\n",
    "        cv2.imshow(\"Combined Feed\", concatenated_frame)\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q'):  # Quit the program\n",
    "            break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-Time with Async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Homography Matrix :  [[   -0.48119     0.96414      561.63]\n",
      " [   -0.29422    -0.19341      630.66]\n",
      " [-0.00038383   0.0015108           1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lq/b9jl13d10hq6lrw9wtvydrj40000gp/T/ipykernel_62184/844599863.py:45: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  return int(image_coords[0]), int(image_coords[1])\n",
      "2025-01-14 11:35:05.002 Python[62184:35468899] +[IMKClient subclass]: chose IMKClient_Legacy\n",
      "2025-01-14 11:35:05.002 Python[62184:35468899] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import threading\n",
    "\n",
    "# Initialize global variables\n",
    "h_matrix = None\n",
    "image_file = \"/Users/xs496-jassin/Desktop/Surveillance/nn.jpg\"\n",
    "image = None\n",
    "trajectory_points = []  # List to store trajectory points on the image\n",
    "threshold = 0.8  # Confidence threshold for person detection\n",
    "\n",
    "class AsyncFrameCapture:\n",
    "    def __init__(self, rtsp_link):\n",
    "        self.rtsp_link = rtsp_link\n",
    "        self.cap = cv2.VideoCapture(rtsp_link)\n",
    "        self.frame = None\n",
    "        self.lock = threading.Lock()\n",
    "        self.condition = threading.Condition(self.lock)\n",
    "        self.running = True\n",
    "        self.thread = threading.Thread(target=self._capture_frames, daemon=True)\n",
    "        self.thread.start()\n",
    "\n",
    "    def _capture_frames(self):\n",
    "        while self.running:\n",
    "            success, frame = self.cap.read()\n",
    "            if success:\n",
    "                with self.lock:\n",
    "                    self.frame = frame\n",
    "                    self.condition.notify_all()\n",
    "\n",
    "    def get_frame(self):\n",
    "        with self.lock:\n",
    "            return self.frame\n",
    "\n",
    "    def stop(self):\n",
    "        self.running = False\n",
    "        self.thread.join()\n",
    "        self.cap.release()\n",
    "\n",
    "def transform_to_image(x, y, h_matrix):\n",
    "    video_coords = np.array([[x, y, 1]]).T  # Homogeneous coordinates\n",
    "    image_coords = np.dot(h_matrix, video_coords)\n",
    "    image_coords /= image_coords[2]  # Normalize by z-coordinate\n",
    "    return int(image_coords[0]), int(image_coords[1])\n",
    "\n",
    "def draw_trajectory(image, points, color=(0, 0, 255), thickness=2):\n",
    "    for i in range(1, len(points)):\n",
    "        cv2.line(image, points[i - 1], points[i], color, thickness)\n",
    "\n",
    "def process_frame(frame, model, threshold, show=True):\n",
    "    results = model.predict(frame, conf=threshold, classes=0, verbose=False)  # Class 0 is 'person'\n",
    "    centers = []\n",
    "\n",
    "    for bbox in results[0].boxes.xyxy:  # Bounding box format: (x1, y1, x2, y2)\n",
    "        x1, y1, x2, y2 = map(int, bbox)\n",
    "        bottom_center = ((x1 + x2) // 2, y2)\n",
    "        centers.append(bottom_center)\n",
    "\n",
    "        if show:\n",
    "            # Draw the bounding box on the frame\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)  # Green box with thickness 2\n",
    "            # Optionally, mark the bottom center point\n",
    "            cv2.circle(frame, bottom_center, 5, (0, 0, 255), -1)  # Red circle\n",
    "\n",
    "    return centers\n",
    "\n",
    "def main():\n",
    "    global image, h_matrix, trajectory_points\n",
    "\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_file)\n",
    "    if image is None:\n",
    "        print(\"Failed to load the image. Check the image path.\")\n",
    "        return\n",
    "\n",
    "    # Load homography matrix\n",
    "    h_matrix = np.loadtxt(\"homography_matrix_grid.txt\")\n",
    "    print(\"Loaded Homography Matrix : \", h_matrix)\n",
    "\n",
    "    # Load YOLO model\n",
    "    model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "    # Open the camera\n",
    "    frame_capture = AsyncFrameCapture(0)\n",
    "    \n",
    "    # Show the image for mapping\n",
    "    # cv2.imshow(\"Mapped Image\", image)\n",
    "\n",
    "    while True:\n",
    "        frame = frame_capture.get_frame() \n",
    "        if frame is None:\n",
    "            continue\n",
    "\n",
    "        # Detect persons and get their bottom-center points\n",
    "        centers = process_frame(frame, model, threshold)\n",
    "\n",
    "        # Map each detected point to the image\n",
    "        for center in centers:\n",
    "            mapped_point = transform_to_image(center[0], center[1], h_matrix)\n",
    "            trajectory_points.append(mapped_point)\n",
    "\n",
    "        # Clear and redraw the trajectory\n",
    "        temp_image = image.copy()\n",
    "        draw_trajectory(temp_image, trajectory_points)\n",
    "\n",
    "        # Resize the frames to the same height for concatenation\n",
    "        frame_height, frame_width = frame.shape[:2]\n",
    "        image_height, image_width = temp_image.shape[:2]\n",
    "\n",
    "        scale = frame_height / image_height\n",
    "        resized_image = cv2.resize(temp_image, (int(image_width * scale), frame_height))\n",
    "\n",
    "        # Horizontally concatenate the video frame and the resized image\n",
    "        concatenated_frame = np.hstack((frame, resized_image))\n",
    "\n",
    "        # Display the concatenated frame\n",
    "        cv2.imshow(\"Combined Feed\", concatenated_frame)\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q'):  # Quit the program\n",
    "            frame_capture.stop()\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without deepsort and with asyc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Homography Matrix:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lq/b9jl13d10hq6lrw9wtvydrj40000gp/T/ipykernel_62184/4103489502.py:28: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  return int(image_coords[0]), int(image_coords[1])\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import numpy as np\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "CONFIDENCE_THRESHOLD = 0.8\n",
    "WHITE = (255, 255, 255)\n",
    "OUTPUT_SIZE = (640, 640)  # New size for both input and output frames\n",
    "\n",
    "# Initialize the video capture object\n",
    "video_cap = cv2.VideoCapture(\"cafe3.mov\")\n",
    "\n",
    "# Load the YOLOv8n model\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "# Load the homography matrix\n",
    "h_matrix = np.loadtxt(\"homography_matrix_grid.txt\")\n",
    "r_image = cv2.imread(\"nn.jpg\")\n",
    "if r_image is None:\n",
    "    raise FileNotFoundError(\"Background image not found at the specified path.\")\n",
    "print(\"Loaded Homography Matrix:\")\n",
    "\n",
    "# Function to transform coordinates using the homography matrix\n",
    "def transform_to_image(x, y, h_matrix):\n",
    "    video_coords = np.array([[x, y, 1]]).T  # Homogeneous coordinates\n",
    "    image_coords = np.dot(h_matrix, video_coords)\n",
    "    image_coords /= image_coords[2]  # Normalize by z-coordinate\n",
    "    return int(image_coords[0]), int(image_coords[1])\n",
    "\n",
    "# AsyncFrameCapture for frame retrieval\n",
    "class AsyncFrameCapture:\n",
    "    def __init__(self, rtsp_link):\n",
    "        self.rtsp_link = rtsp_link\n",
    "        self.cap = cv2.VideoCapture(rtsp_link)\n",
    "        self.frame = None\n",
    "        self.lock = threading.Lock()\n",
    "        self.condition = threading.Condition(self.lock)\n",
    "        self.running = True\n",
    "        self.thread = threading.Thread(target=self._capture_frames, daemon=True)\n",
    "        self.thread.start()\n",
    "\n",
    "    def _capture_frames(self):\n",
    "        while self.running:\n",
    "            success, frame = self.cap.read()\n",
    "            frame = cv2.resize(frame, (640,640))\n",
    "            if success:\n",
    "                with self.lock:\n",
    "                    self.frame = frame\n",
    "                    self.condition.notify_all()\n",
    "\n",
    "    def get_frame(self):\n",
    "        with self.lock:\n",
    "            return self.frame\n",
    "\n",
    "    def stop(self):\n",
    "        self.running = False\n",
    "        self.thread.join()\n",
    "        self.cap.release()\n",
    "\n",
    "frame_capture = AsyncFrameCapture(\"/Users/xs496-jassin/Desktop/Surveillance/cafe3.mov\")\n",
    "\n",
    "while True:\n",
    "    frame = frame_capture.get_frame()\n",
    "    if frame is None:\n",
    "        continue\n",
    "\n",
    "    # Run the YOLO model on the frame\n",
    "    detections = model.predict(frame, conf=CONFIDENCE_THRESHOLD, verbose=False)[0]\n",
    "\n",
    "    # Prepare detection results\n",
    "    for data in detections.boxes.data.tolist():\n",
    "        confidence = data[4]\n",
    "        class_id = int(data[5])\n",
    "        if class_id != 0 or confidence < CONFIDENCE_THRESHOLD:  # Skip non-person detections\n",
    "            continue\n",
    "\n",
    "        xmin, ymin, xmax, ymax = int(data[0]), int(data[1]), int(data[2]), int(data[3])\n",
    "        x, y = (xmin + xmax) // 2, ymax\n",
    "\n",
    "        # Transform the center point to the room image\n",
    "        mapped_point = transform_to_image(x, y, h_matrix)\n",
    "\n",
    "        # Draw bounding box and center point\n",
    "        color = (255,0,255)\n",
    "        cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), color, 2)\n",
    "        cv2.circle(frame, (x, y), 5, color, -1)\n",
    "\n",
    "        # Draw mapped point on the room image\n",
    "        cv2.circle(r_image, mapped_point, 5, color, -1)\n",
    "\n",
    "    # Resize and display frames\n",
    "    frame_resized_original = cv2.resize(r_image, OUTPUT_SIZE)\n",
    "    frame_resized = cv2.resize(frame, OUTPUT_SIZE)\n",
    "    stacked_frame = np.hstack((frame_resized_original, frame_resized))\n",
    "    cv2.imshow(\"Frame Stack\", stacked_frame)\n",
    "\n",
    "    if cv2.waitKey(1) == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "frame_capture.stop()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With DeepSORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Homography Matrix:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-13 20:26:53.706 Python[22828:32963192] +[IMKClient subclass]: chose IMKClient_Legacy\n",
      "2025-01-13 20:26:53.706 Python[22828:32963192] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n",
      "/var/folders/lq/b9jl13d10hq6lrw9wtvydrj40000gp/T/ipykernel_22828/4015661398.py:61: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  return int(image_coords[0]), int(image_coords[1])\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "\n",
    "CONFIDENCE_THRESHOLD = 0.8\n",
    "WHITE = (255, 255, 255)\n",
    "OUTPUT_SIZE = (640, 640)  # New size for both input and output frames\n",
    "\n",
    "# Initialize the video capture object\n",
    "video_cap = cv2.VideoCapture(\"/Users/xs496-jassin/Desktop/Surveillance/cafe3.mov\")\n",
    "\n",
    "# Load the YOLOv8n model\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "tracker = DeepSort(max_age=50)\n",
    "\n",
    "# Load the homography matrix\n",
    "h_matrix = np.loadtxt(\"homography_matrix_grid.txt\")\n",
    "r_image = cv2.imread(\"/Users/xs496-jassin/Desktop/Surveillance/nn.jpg\")\n",
    "if r_image is None:\n",
    "    raise FileNotFoundError(\"Background image not found at the specified path.\")\n",
    "print(\"Loaded Homography Matrix:\")\n",
    "\n",
    "# Dictionary to store trajectory points for each track ID\n",
    "trajectory_points = {}\n",
    "\n",
    "# Function to draw dashed lines\n",
    "def draw_dashed_line(image, start_point, end_point, color, thickness, dash_length=10):\n",
    "    \"\"\"Draws a dashed line between two points.\"\"\"\n",
    "    x1, y1 = start_point\n",
    "    x2, y2 = end_point\n",
    "    line_length = int(np.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2))\n",
    "    for i in range(0, line_length, dash_length * 2):\n",
    "        start = i / line_length\n",
    "        end = (i + dash_length) / line_length\n",
    "        start_x = int(x1 + start * (x2 - x1))\n",
    "        start_y = int(y1 + start * (y2 - y1))\n",
    "        end_x = int(x1 + end * (x2 - x1))\n",
    "        end_y = int(y1 + end * (y2 - y1))\n",
    "        cv2.line(image, (start_x, start_y), (end_x, end_y), color, thickness)\n",
    "\n",
    "def draw_dotted_line(image, start_point, end_point, color, radius=3):\n",
    "    \"\"\"Draws a line with dots between two points.\"\"\"\n",
    "    x1, y1 = start_point\n",
    "    x2, y2 = end_point\n",
    "    line_length = int(np.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2))\n",
    "    \n",
    "    # Place dots along the line at regular intervals\n",
    "    for i in range(0, line_length, radius * 4):  # Adjust spacing by modifying radius * 4\n",
    "        start = i / line_length\n",
    "        start_x = int(x1 + start * (x2 - x1))\n",
    "        start_y = int(y1 + start * (y2 - y1))\n",
    "        cv2.circle(image, (start_x, start_y), radius, color, -1)\n",
    "\n",
    "# Function to transform coordinates using the homography matrix\n",
    "def transform_to_image(x, y, h_matrix):\n",
    "    video_coords = np.array([[x, y, 1]]).T  # Homogeneous coordinates\n",
    "    image_coords = np.dot(h_matrix, video_coords)\n",
    "    image_coords /= image_coords[2]  # Normalize by z-coordinate\n",
    "    return int(image_coords[0]), int(image_coords[1])\n",
    "\n",
    "# Function to generate a unique color for each track ID\n",
    "def get_unique_color(track_id):\n",
    "    try:\n",
    "        track_id = int(track_id)  # Ensure track_id is an integer\n",
    "    except ValueError:\n",
    "        raise TypeError(f\"Track ID must be convertible to an integer, but got {track_id}\")\n",
    "\n",
    "    np.random.seed(track_id)  # Seed the random number generator\n",
    "    color = tuple(np.random.randint(0, 255, 3).tolist())\n",
    "    return color\n",
    "\n",
    "while True:\n",
    "    ret, frame = video_cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Run the YOLO model on the frame\n",
    "    detections = model.predict(frame, conf=CONFIDENCE_THRESHOLD, verbose=False)[0]\n",
    "\n",
    "    # Prepare detection results\n",
    "    results = []\n",
    "    for data in detections.boxes.data.tolist():\n",
    "        confidence = data[4]\n",
    "        class_id = int(data[5])\n",
    "        if class_id != 0 or confidence < CONFIDENCE_THRESHOLD:  # Skip non-person detections\n",
    "            continue\n",
    "        xmin, ymin, xmax, ymax = int(data[0]), int(data[1]), int(data[2]), int(data[3])\n",
    "        results.append([[xmin, ymin, xmax - xmin, ymax - ymin], confidence, class_id])\n",
    "\n",
    "    # Update the tracker\n",
    "    tracks = tracker.update_tracks(results, frame=frame)\n",
    "\n",
    "    # Draw detections and trajectories\n",
    "    for track in tracks:\n",
    "        if not track.is_confirmed():\n",
    "            continue\n",
    "\n",
    "        track_id = track.track_id\n",
    "        ltrb = track.to_ltrb()\n",
    "        xmin, ymin, xmax, ymax = int(ltrb[0]), int(ltrb[1]), int(ltrb[2]), int(ltrb[3])\n",
    "        x, y = (xmin + xmax) // 2, ymax  # Bottom-center points\n",
    "        # center_y = ymin + (ymax - ymin) // 2  # Center of the bounding box\n",
    "        # y = (ymax + center_y) // 2  # Midpoint between the bottom and the center\n",
    "        # x = (xmin + xmax) // 2  \n",
    "\n",
    "        # Get unique color for the track\n",
    "        color = get_unique_color(track_id)\n",
    "\n",
    "        # Map the point to the room image\n",
    "        mapped_point = transform_to_image(x, y, h_matrix)\n",
    "\n",
    "        # Update the trajectory for this track\n",
    "        if track_id not in trajectory_points:\n",
    "            trajectory_points[track_id] = []\n",
    "        trajectory_points[track_id].append(mapped_point)\n",
    "\n",
    "        # Draw the dashed trajectory line\n",
    "        points = trajectory_points[track_id]\n",
    "        for i in range(1, len(points)):\n",
    "            draw_dotted_line(r_image, points[i - 1], points[i], color)\n",
    "\n",
    "        # Draw bounding box and track ID\n",
    "        cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), color, 2)\n",
    "        cv2.putText(frame, str(track_id), (xmin + 5, ymin - 8),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, WHITE, 2)\n",
    "\n",
    "    # Resize images for display\n",
    "    frame_resized_original = cv2.resize(r_image, OUTPUT_SIZE)\n",
    "    frame_resized = cv2.resize(frame, OUTPUT_SIZE)\n",
    "\n",
    "    # Stack frames side by side\n",
    "    stacked_frame = np.hstack((frame_resized_original, frame_resized))\n",
    "\n",
    "    # Show the stacked frames\n",
    "    cv2.imshow(\"Frame Stack\", stacked_frame)\n",
    "    if cv2.waitKey(1) == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "video_cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try Fast (with AsyncFrameCapture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Homography Matrix:\n"
     ]
    }
   ],
   "source": [
    "# import datetime\n",
    "# import numpy as np\n",
    "# import cv2\n",
    "# import threading\n",
    "# from ultralytics import YOLO\n",
    "# from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "\n",
    "# CONFIDENCE_THRESHOLD = 0.1\n",
    "# WHITE = (255, 255, 255)\n",
    "# OUTPUT_SIZE = (640, 640)  # New size for both input and output frames\n",
    "\n",
    "# # Initialize the YOLO model and tracker\n",
    "# model = YOLO(\"yolov8n.pt\")\n",
    "# tracker = DeepSort(max_age=50)\n",
    "\n",
    "# # Load the homography matrix\n",
    "# h_matrix = np.loadtxt(\"homography_matrix_cafeteria.txt\")\n",
    "# r_image = cv2.imread(\"/Users/xs496-jassin/Desktop/Surveillance/cafeteria2.jpg\")\n",
    "# if r_image is None:\n",
    "#     raise FileNotFoundError(\"Background image not found at the specified path.\")\n",
    "# print(\"Loaded Homography Matrix:\")\n",
    "\n",
    "# # Initialize trajectory points dictionary\n",
    "# trajectory_points = {}\n",
    "\n",
    "# # Function to draw dashed lines\n",
    "# def draw_dashed_line(image, start_point, end_point, color, thickness, dash_length=10):\n",
    "#     \"\"\"Draws a dashed line between two points.\"\"\"\n",
    "#     x1, y1 = start_point\n",
    "#     x2, y2 = end_point\n",
    "#     line_length = int(np.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2))\n",
    "#     for i in range(0, line_length, dash_length * 2):\n",
    "#         start = i / line_length\n",
    "#         end = (i + dash_length) / line_length\n",
    "#         start_x = int(x1 + start * (x2 - x1))\n",
    "#         start_y = int(y1 + start * (y2 - y1))\n",
    "#         end_x = int(x1 + end * (x2 - x1))\n",
    "#         end_y = int(y1 + end * (y2 - y1))\n",
    "#         cv2.line(image, (start_x, start_y), (end_x, end_y), color, thickness)\n",
    "\n",
    "# def draw_dotted_line(image, start_point, end_point, color, radius=3):\n",
    "#     \"\"\"Draws a line with dots between two points.\"\"\"\n",
    "#     x1, y1 = start_point\n",
    "#     x2, y2 = end_point\n",
    "#     line_length = int(np.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2))\n",
    "    \n",
    "#     # Place dots along the line at regular intervals\n",
    "#     for i in range(0, line_length, radius * 4):  # Adjust spacing by modifying radius * 4\n",
    "#         start = i / line_length\n",
    "#         start_x = int(x1 + start * (x2 - x1))\n",
    "#         start_y = int(y1 + start * (y2 - y1))\n",
    "#         cv2.circle(image, (start_x, start_y), radius, color, -1)\n",
    "\n",
    "# # Function to transform coordinates using the homography matrix\n",
    "# def transform_to_image(x, y, h_matrix):\n",
    "#     video_coords = np.array([[x, y, 1]]).T  # Homogeneous coordinates\n",
    "#     image_coords = np.dot(h_matrix, video_coords)\n",
    "#     image_coords /= image_coords[2]  # Normalize by z-coordinate\n",
    "#     return int(image_coords[0]), int(image_coords[1])\n",
    "\n",
    "# # Function to generate a unique color for each track ID\n",
    "# def get_unique_color(track_id):\n",
    "#     try:\n",
    "#         track_id = int(track_id)  # Ensure track_id is an integer\n",
    "#     except ValueError:\n",
    "#         raise TypeError(f\"Track ID must be convertible to an integer, but got {track_id}\")\n",
    "\n",
    "#     np.random.seed(track_id)  # Seed the random number generator\n",
    "#     color = tuple(np.random.randint(0, 255, 3).tolist())\n",
    "#     return color\n",
    "\n",
    "# # AsyncFrameCapture for frame retrieval\n",
    "# class AsyncFrameCapture:\n",
    "#     def __init__(self, rtsp_link):\n",
    "#         self.rtsp_link = rtsp_link\n",
    "#         self.cap = cv2.VideoCapture(rtsp_link)\n",
    "#         self.frame = None\n",
    "#         self.lock = threading.Lock()\n",
    "#         self.condition = threading.Condition(self.lock)\n",
    "#         self.running = True\n",
    "#         self.thread = threading.Thread(target=self._capture_frames, daemon=True)\n",
    "#         self.thread.start()\n",
    "\n",
    "#     def _capture_frames(self):\n",
    "#         while self.running:\n",
    "#             success, frame = self.cap.read()\n",
    "#             if success:\n",
    "#                 with self.lock:\n",
    "#                     self.frame = frame\n",
    "#                     self.condition.notify_all()\n",
    "\n",
    "#     def get_frame(self):\n",
    "#         with self.lock:\n",
    "#             return self.frame\n",
    "\n",
    "#     def stop(self):\n",
    "#         self.running = False\n",
    "#         self.thread.join()\n",
    "#         self.cap.release()\n",
    "\n",
    "# # Initialize frame capture\n",
    "# frame_capture = AsyncFrameCapture(\"/Users/xs496-jassin/Desktop/Surveillance/cafe3.mov\")\n",
    "\n",
    "# # Main processing loop\n",
    "# while True:\n",
    "#     # Get the latest frame\n",
    "#     frame = frame_capture.get_frame()\n",
    "#     if frame is None:\n",
    "#         continue\n",
    "\n",
    "#     # Run the YOLO model on the frame\n",
    "#     detections = model.predict(frame, conf=CONFIDENCE_THRESHOLD, verbose=False)[0]\n",
    "\n",
    "#     # Prepare detection results\n",
    "#     results = []\n",
    "#     for data in detections.boxes.data.tolist():\n",
    "#         confidence = data[4]\n",
    "#         class_id = int(data[5])\n",
    "#         if class_id != 0 or confidence < CONFIDENCE_THRESHOLD:\n",
    "#             continue\n",
    "#         xmin, ymin, xmax, ymax = int(data[0]), int(data[1]), int(data[2]), int(data[3])\n",
    "#         results.append([[xmin, ymin, xmax - xmin, ymax - ymin], confidence, class_id])\n",
    "\n",
    "#     # Update the tracker\n",
    "#     tracks = tracker.update_tracks(results, frame=frame)\n",
    "\n",
    "#     # Draw detections and trajectories\n",
    "#     for track in tracks:\n",
    "#         if not track.is_confirmed():\n",
    "#             continue\n",
    "\n",
    "#         track_id = track.track_id\n",
    "#         ltrb = track.to_ltrb()\n",
    "#         xmin, ymin, xmax, ymax = int(ltrb[0]), int(ltrb[1]), int(ltrb[2]), int(ltrb[3])\n",
    "#         center_y = ymin + (ymax - ymin) // 2  # Center of the bounding box\n",
    "#         y = (ymax + center_y) // 2  # Midpoint between the bottom and the center\n",
    "#         x = (xmin + xmax) // 2  \n",
    "\n",
    "#         # Get unique color for the track\n",
    "#         color = get_unique_color(track_id)\n",
    "\n",
    "#         # Map the point to the room image\n",
    "#         mapped_point = transform_to_image(x, y, h_matrix)\n",
    "\n",
    "#         # Update the trajectory for this track\n",
    "#         if track_id not in trajectory_points:\n",
    "#             trajectory_points[track_id] = []\n",
    "#         trajectory_points[track_id].append(mapped_point)\n",
    "\n",
    "#         # Draw the dashed trajectory line\n",
    "#         points = trajectory_points[track_id]\n",
    "#         for i in range(1, len(points)):\n",
    "#             draw_dotted_line(r_image, points[i - 1], points[i], color)\n",
    "\n",
    "#         # Draw bounding box and track ID\n",
    "#         cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), color, 2)\n",
    "#         cv2.putText(frame, str(track_id), (xmin + 5, ymin - 8),\n",
    "#                     cv2.FONT_HERSHEY_SIMPLEX, 0.5, WHITE, 2)\n",
    "\n",
    "#     # Resize images for display\n",
    "#     frame_resized_original = cv2.resize(r_image, OUTPUT_SIZE)\n",
    "#     frame_resized = cv2.resize(frame, OUTPUT_SIZE)\n",
    "\n",
    "#     # Stack frames side by side\n",
    "#     stacked_frame = np.hstack((frame_resized_original, frame_resized))\n",
    "\n",
    "#     # Show the stacked frames\n",
    "#     cv2.imshow(\"Frame Stack\", stacked_frame)\n",
    "#     if cv2.waitKey(1) == ord(\"q\"):\n",
    "#         break\n",
    "\n",
    "# # Stop the frame capture and clean up\n",
    "# frame_capture.stop()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Homography Matrix:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-14 09:33:55.127 Python[87020:35223843] +[IMKClient subclass]: chose IMKClient_Legacy\n",
      "2025-01-14 09:33:55.127 Python[87020:35223843] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import numpy as np\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "\n",
    "CONFIDENCE_THRESHOLD = 0.8\n",
    "WHITE = (255, 255, 255)\n",
    "OUTPUT_SIZE = (640, 640)  # New size for both input and output frames\n",
    "\n",
    "# Initialize the video capture object\n",
    "video_cap = cv2.VideoCapture(\"cafe3.mov\")\n",
    "\n",
    "# Load the YOLOv8n model\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "tracker = DeepSort(max_age=50)\n",
    "\n",
    "# Load the homography matrix\n",
    "h_matrix = np.loadtxt(\"homography_matrix_grid.txt\")\n",
    "r_image = cv2.imread(\"cafeteria2.jpg\")\n",
    "if r_image is None:\n",
    "    raise FileNotFoundError(\"Background image not found at the specified path.\")\n",
    "print(\"Loaded Homography Matrix:\")\n",
    "\n",
    "# Dictionary to store trajectory points for each track ID\n",
    "trajectory_points = {}\n",
    "\n",
    "# AsyncFrameCapture for frame retrieval\n",
    "class AsyncFrameCapture:\n",
    "    def __init__(self, rtsp_link):\n",
    "        self.rtsp_link = rtsp_link\n",
    "        self.cap = cv2.VideoCapture(rtsp_link)\n",
    "        self.frame = None\n",
    "        self.lock = threading.Lock()\n",
    "        self.condition = threading.Condition(self.lock)\n",
    "        self.running = True\n",
    "        self.thread = threading.Thread(target=self._capture_frames, daemon=True)\n",
    "        self.thread.start()\n",
    "\n",
    "    def _capture_frames(self):\n",
    "        while self.running:\n",
    "            success, frame = self.cap.read()\n",
    "            if success:\n",
    "                with self.lock:\n",
    "                    self.frame = frame\n",
    "                    self.condition.notify_all()\n",
    "\n",
    "    def get_frame(self):\n",
    "        with self.lock:\n",
    "            return self.frame\n",
    "\n",
    "    def stop(self):\n",
    "        self.running = False\n",
    "        self.thread.join()\n",
    "        self.cap.release()\n",
    "\n",
    "# Function to draw dashed lines\n",
    "def draw_dashed_line(image, start_point, end_point, color, thickness, dash_length=10):\n",
    "    \"\"\"Draws a dashed line between two points.\"\"\"\n",
    "    x1, y1 = start_point\n",
    "    x2, y2 = end_point\n",
    "    line_length = int(np.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2))\n",
    "    for i in range(0, line_length, dash_length * 2):\n",
    "        start = i / line_length\n",
    "        end = (i + dash_length) / line_length\n",
    "        start_x = int(x1 + start * (x2 - x1))\n",
    "        start_y = int(y1 + start * (y2 - y1))\n",
    "        end_x = int(x1 + end * (x2 - x1))\n",
    "        end_y = int(y1 + end * (y2 - y1))\n",
    "        cv2.line(image, (start_x, start_y), (end_x, end_y), color, thickness)\n",
    "\n",
    "def draw_dotted_line(image, start_point, end_point, color, radius=3):\n",
    "    \"\"\"Draws a line with dots between two points.\"\"\"\n",
    "    x1, y1 = start_point\n",
    "    x2, y2 = end_point\n",
    "    line_length = int(np.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2))\n",
    "    \n",
    "    # Place dots along the line at regular intervals\n",
    "    for i in range(0, line_length, radius * 4):  # Adjust spacing by modifying radius * 4\n",
    "        start = i / line_length\n",
    "        start_x = int(x1 + start * (x2 - x1))\n",
    "        start_y = int(y1 + start * (y2 - y1))\n",
    "        cv2.circle(image, (start_x, start_y), radius, color, -1)\n",
    "\n",
    "# Function to transform coordinates using the homography matrix\n",
    "def transform_to_image(x, y, h_matrix):\n",
    "    video_coords = np.array([[x, y, 1]]).T  # Homogeneous coordinates\n",
    "    image_coords = np.dot(h_matrix, video_coords)\n",
    "    image_coords /= image_coords[2]  # Normalize by z-coordinate\n",
    "    return int(image_coords[0]), int(image_coords[1])\n",
    "\n",
    "# Function to generate a unique color for each track ID\n",
    "def get_unique_color(track_id):\n",
    "    try:\n",
    "        track_id = int(track_id)  # Ensure track_id is an integer\n",
    "    except ValueError:\n",
    "        raise TypeError(f\"Track ID must be convertible to an integer, but got {track_id}\")\n",
    "\n",
    "    np.random.seed(track_id)  # Seed the random number generator\n",
    "    color = tuple(np.random.randint(0, 255, 3).tolist())\n",
    "    return color\n",
    "\n",
    "frame_capture = AsyncFrameCapture(\"/Users/xs496-jassin/Desktop/Surveillance/cafe3.mov\")\n",
    "\n",
    "while True:\n",
    "    frame = frame_capture.get_frame()\n",
    "    if frame is None:\n",
    "        continue\n",
    "\n",
    "    # Run the YOLO model on the frame\n",
    "    detections = model.predict(frame, conf=CONFIDENCE_THRESHOLD, verbose=False)[0]\n",
    "\n",
    "    # Prepare detection results\n",
    "    results = []\n",
    "    for data in detections.boxes.data.tolist():\n",
    "        confidence = data[4]\n",
    "        class_id = int(data[5])\n",
    "        if class_id != 0 or confidence < CONFIDENCE_THRESHOLD:  # Skip non-person detections\n",
    "            continue\n",
    "        xmin, ymin, xmax, ymax = int(data[0]), int(data[1]), int(data[2]), int(data[3])\n",
    "        results.append([[xmin, ymin, xmax - xmin, ymax - ymin], confidence, class_id])\n",
    "\n",
    "    # Update the tracker\n",
    "    tracks = tracker.update_tracks(results, frame=frame)\n",
    "\n",
    "    # Draw detections and trajectories\n",
    "    for track in tracks:\n",
    "        if not track.is_confirmed():\n",
    "            continue\n",
    "\n",
    "        track_id = track.track_id\n",
    "        ltrb = track.to_ltrb()\n",
    "        xmin, ymin, xmax, ymax = int(ltrb[0]), int(ltrb[1]), int(ltrb[2]), int(ltrb[3])\n",
    "        # x, y = (xmin + xmax) // 2, ymax  # Bottom-center point\n",
    "        center_y = ymin + (ymax - ymin) // 2  # Center of the bounding box\n",
    "        y = (ymax + center_y) // 2  # Midpoint between the bottom and the center\n",
    "        x = (xmin + xmax) // 2  \n",
    "\n",
    "        # Get unique color for the track\n",
    "        color = get_unique_color(track_id)\n",
    "\n",
    "        # Map the point to the room image\n",
    "        mapped_point = transform_to_image(x, y, h_matrix)\n",
    "\n",
    "        # Update the trajectory for this track\n",
    "        if track_id not in trajectory_points:\n",
    "            trajectory_points[track_id] = []\n",
    "        trajectory_points[track_id].append(mapped_point)\n",
    "\n",
    "        # Draw the dashed trajectory line\n",
    "        points = trajectory_points[track_id]\n",
    "        for i in range(1, len(points)):\n",
    "            draw_dotted_line(r_image, points[i - 1], points[i], color)\n",
    "\n",
    "        # Draw bounding box and track ID\n",
    "        cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), color, 2)\n",
    "        cv2.putText(frame, str(track_id), (xmin + 5, ymin - 8),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, WHITE, 2)\n",
    "\n",
    "    # Resize images for display\n",
    "    frame_resized_original = cv2.resize(r_image, OUTPUT_SIZE)\n",
    "    frame_resized = cv2.resize(frame, OUTPUT_SIZE)\n",
    "\n",
    "    # Stack frames side by side\n",
    "    stacked_frame = np.hstack((frame_resized_original, frame_resized))\n",
    "\n",
    "    # Show the stacked frames\n",
    "    cv2.imshow(\"Frame Stack\", stacked_frame)\n",
    "    if cv2.waitKey(1) == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "frame_capture.stop()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From ykw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Homography Matrix:\n",
      "[[   -0.47961     0.63736      755.06]\n",
      " [-0.00090173    -0.26667      431.41]\n",
      " [-0.00017298   0.0014242           1]]\n",
      "\n",
      "0: 384x640 1 person, 72.3ms\n",
      "Speed: 2.5ms preprocess, 72.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'float' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 130\u001b[0m\n\u001b[1;32m    126\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 130\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 93\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m     confidences\u001b[38;5;241m.\u001b[39mappend(score)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Update tracker\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m tracks \u001b[38;5;241m=\u001b[39m \u001b[43mtracker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_tracks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbbox_xywh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfidences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Process each track\u001b[39;00m\n\u001b[1;32m     96\u001b[0m temp_image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m~/Desktop/Surveillance/cctv/lib/python3.9/site-packages/deep_sort_realtime/deepsort_tracker.py:195\u001b[0m, in \u001b[0;36mDeepSort.update_tracks\u001b[0;34m(self, raw_detections, embeds, frame, today, others, instance_masks)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(raw_detections) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolygon:\n\u001b[0;32m--> 195\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mraw_detections\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m4\u001b[39m\n\u001b[1;32m    196\u001b[0m         raw_detections \u001b[38;5;241m=\u001b[39m [d \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m raw_detections \u001b[38;5;28;01mif\u001b[39;00m d[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m d[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'float' has no len()"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import random\n",
    "\n",
    "# Initialize global variables\n",
    "h_matrix = None\n",
    "image_file = \"room.jpg\"\n",
    "image = None\n",
    "threshold = 0.5  # Confidence threshold for person detection\n",
    "tracker = DeepSort(max_age=30, n_init=3, max_cosine_distance=0.4, nn_budget=None)\n",
    "person_colors = {}  # Dictionary to store unique colors for each tracked ID\n",
    "\n",
    "\n",
    "def transform_to_image(x, y, h_matrix):\n",
    "    \"\"\"Transform video coordinates to image coordinates using the homography matrix.\"\"\"\n",
    "    video_coords = np.array([[x, y, 1]]).T  # Homogeneous coordinates\n",
    "    image_coords = np.dot(h_matrix, video_coords)\n",
    "    image_coords /= image_coords[2]  # Normalize by z-coordinate\n",
    "    return int(image_coords[0]), int(image_coords[1])\n",
    "\n",
    "\n",
    "def draw_trajectory(image, points, color=(0, 0, 255), thickness=2):\n",
    "    \"\"\"Draw a trajectory connecting the points.\"\"\"\n",
    "    for i in range(1, len(points)):\n",
    "        cv2.line(image, points[i - 1], points[i], color, thickness)\n",
    "\n",
    "\n",
    "def process_frame(frame, model, threshold):\n",
    "    \"\"\"Detect persons in the frame and return the bounding boxes and scores.\"\"\"\n",
    "    results = model.predict(frame, conf=threshold, classes=0)  # Class 0 is 'person'\n",
    "    detections = []\n",
    "\n",
    "    for bbox, score in zip(results[0].boxes.xyxy, results[0].boxes.conf):\n",
    "        x1, y1, x2, y2 = map(int, bbox)\n",
    "        detections.append(((x1, y1, x2, y2), score))\n",
    "\n",
    "    return detections\n",
    "\n",
    "\n",
    "def assign_color(tracked_id):\n",
    "    \"\"\"Assign a unique color to each tracked ID.\"\"\"\n",
    "    if tracked_id not in person_colors:\n",
    "        person_colors[tracked_id] = [random.randint(0, 255) for _ in range(3)]\n",
    "    return person_colors[tracked_id]\n",
    "\n",
    "\n",
    "def main():\n",
    "    global image, h_matrix\n",
    "\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_file)\n",
    "    if image is None:\n",
    "        print(\"Failed to load the image. Check the image path.\")\n",
    "        return\n",
    "\n",
    "    # Load homography matrix\n",
    "    h_matrix = np.loadtxt(\"homography_matrix_ai_room.txt\")  # Load the previously saved homography matrix\n",
    "    print(\"Loaded Homography Matrix:\")\n",
    "    print(h_matrix)\n",
    "\n",
    "    # Load YOLO model\n",
    "    model = YOLO(\"yolov8n.pt\")  # Replace with your lightweight YOLO model\n",
    "\n",
    "    # Open the camera\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Failed to open the camera.\")\n",
    "        return\n",
    "\n",
    "    # Show the image for mapping\n",
    "    cv2.imshow(\"Mapped Image\", image)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to grab frame from the camera.\")\n",
    "            break\n",
    "\n",
    "        # Detect persons and get their bounding boxes and scores\n",
    "        detections = process_frame(frame, model, threshold)\n",
    "\n",
    "        # Prepare detections for DeepSORT\n",
    "        bbox_xywh = []\n",
    "        confidences = []\n",
    "        for bbox, score in detections:\n",
    "            x1, y1, x2, y2 = bbox\n",
    "            bbox_xywh.append([(x1 + x2) / 2, (y1 + y2) / 2, x2 - x1, y2 - y1])\n",
    "            confidences.append(score)\n",
    "\n",
    "        # Update tracker\n",
    "        tracks = tracker.update_tracks(bbox_xywh, confidences, frame)\n",
    "\n",
    "        # Process each track\n",
    "        temp_image = image.copy()\n",
    "        for track in tracks:\n",
    "            if not track.is_confirmed():\n",
    "                continue\n",
    "\n",
    "            track_id = track.track_id\n",
    "            bbox = track.to_ltrb()  # Convert to (x1, y1, x2, y2)\n",
    "            x1, y1, x2, y2 = map(int, bbox)\n",
    "            cx, cy = int((x1 + x2) / 2), int(y2)  # Bottom-center point\n",
    "\n",
    "            # Map the tracked point to the image\n",
    "            mapped_point = transform_to_image(cx, cy, h_matrix)\n",
    "\n",
    "            # Draw the bounding box and ID on the frame\n",
    "            color = assign_color(track_id)\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "            cv2.putText(frame, f\"ID: {track_id}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "            # Draw the trajectory\n",
    "            draw_trajectory(temp_image, [mapped_point], color=color, thickness=2)\n",
    "\n",
    "        # Display the updated image and video feed\n",
    "        cv2.imshow(\"Mapped Image\", temp_image)\n",
    "        cv2.imshow(\"Camera Feed\", frame)\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q'):  # Quit the program\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cctv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
